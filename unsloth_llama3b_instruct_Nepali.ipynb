{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1f8skuLHCZ1SeiaeRKVOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajeeb321123/Ai-chat-buddy/blob/master/unsloth_llama3b_instruct_Nepali.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxbvEb7hog7Y",
        "outputId": "91c57acf-b185-4890-8f4a-99fbdc299717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.41.2)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.25.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.19.2)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->trl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.23.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (4.66.4)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "24fbd8e3388949d496e3555f82d0602a",
            "66bfc5db81f44c96bcb0938619c376b5",
            "ca0740658f87493791f9bbff2434b59a",
            "03288561a1ad4874a40a6654d8ceac9f",
            "ce69dff43498411fbd1692756aa8ad4f",
            "f97889e6c3a34bdfab6704347c8a0e5d",
            "7794b2d527ed4c3390da1899cf035aa8",
            "400172960ef64a8abeea866fe9aec316",
            "77c7fec1e3284ad5be504c084d7c60b0",
            "c94c95c002714954903ac42b362c2458",
            "929d0e4a1dfb4f8dab8907759258441a",
            "9657281edd1f4242af0409ac66c84f9d",
            "d56732da295b4711b0597884d5613fe6",
            "e6a8f4f5053a43108c1ca893ec0b95ce",
            "ed037de2addf48ac988e0091e8c24962",
            "9062a44498354d2b830161c91956ed84",
            "350e935111ad40a6a3b6040f518725cc",
            "a923340f7ad14c2b9ee1ffe90fb7029f",
            "0ed4a0ffca59419db3fcb287b59fb711",
            "e6089b98dd8247dda0639ea80f0e041c",
            "5b598adda30f42f19e4548e15ce578d3",
            "f63a7ed444074f99b7c1e12510044148",
            "74ba3a447a1340c3839ec3390d16adb0",
            "ccb3b60414f842979fc5c614ef87292a",
            "175e55bbef694364ac220d5ae42796b2",
            "cfd9818ff1544371a3c324fd6cc20c24",
            "ae790984b8c640fd94d152946330bee8",
            "8c4991fb4dfc460b9aa881b9f54f2d8e",
            "fc175099611e4de8ae0260ed4b193069",
            "970ada2006ce4a3289b3db2dd3ed909b",
            "35a763e4bb8a44e1834869945cf1e77e",
            "0931b7eca8034405a61a8950d1fe1ae4",
            "6f3655b444354feb97de4d9467293dd1",
            "7b020e1f9de44cdbb729ca869ccaab3e",
            "379f5a20afab40be8ac0742f2af458e3",
            "b56000eb783f4fb2987731f13e882b55",
            "21f09cce299842178e5e9e0e1297e68d",
            "b9d7ada35fc3499db7017d531044cb4f",
            "ff086184169242d9bbb07c969896eb85",
            "790db3dd620e48aba2fe1302d9ba35ca",
            "d9d8b634386e4bd5b95d25aa510fd9b9",
            "6bb9fc745ec14dd49a2803934f5bcf91",
            "340e66ecfb0e416ea0b5295434e103f7",
            "2c9f5f7845e940aaa91dfa6977ce63e7",
            "115ca412c237404c8f58e082aaca9994",
            "27567fe9466a42458c35c4d96f99bbf9",
            "ee67b600f7414f1599e30ac7beb99708",
            "b94afd7e1f0e456a98046d317e882b94",
            "a1243c6c650244e1a3be4778038b9007",
            "072ecafd687549039c5a28232d0b971b",
            "4b990e90175249919f8e4e80a4e83400",
            "5ed2e1a5d8934ee684a435dafbe4bd2f",
            "122ed11870634768a34231f8a8ec371f",
            "45e7f84072fa4a43b567df4b15de9f3d",
            "a54245c4d8d64f89903bf57b6b3eced0",
            "5a3c8b5d41ac42b6ad6dbdbc3d88c4d4",
            "706c192c01074357af067d52594bd9fa",
            "309a6f240906495bbe1711b3c0b7113b",
            "43ac414f21d8492cb676d476db998382",
            "9b274f6388794b05ba1880940852fb5a",
            "1845b336837a466cb5c6fea3051e7749",
            "07cc2e884c9d4a1e9a1ec8ac609bbf9a",
            "71539d5bcb194e6ea907c2236f24384d",
            "670d30c8d4a649dcbfc00246df267940",
            "80eb2d31ab624cf19f8fcab3bda900a2",
            "1a8dae0942c94e6b9d8d19ef868029c9"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "426390b4-f9d7-4cf6-ab90-f3f507b2c831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24fbd8e3388949d496e3555f82d0602a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9657281edd1f4242af0409ac66c84f9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74ba3a447a1340c3839ec3390d16adb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b020e1f9de44cdbb729ca869ccaab3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "115ca412c237404c8f58e082aaca9994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a3c8b5d41ac42b6ad6dbdbc3d88c4d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! Llama 3 is up to 8k\n",
        "dtype = None\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYT0oo2QiiEU",
        "outputId": "02020d39-82a9-4354-f3d0-2e1b34248a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B7NxXMhYTdh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we integrate LoRA adapters into our model, which allows us to efficiently update just a fraction of the model's parameters, enhancing training speed and reducing computational load."
      ],
      "metadata": {
        "id": "SXd9bTZd1aaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe293a1-cad3-4ef6-fc0c-5c97aca84fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "Then, we define a system prompt that formats tasks into instructions, inputs, and responses, and apply it to a dataset to prepare our inputs and outputs for the model, with an EOS token to signal completion.\n"
      ],
      "metadata": {
        "id": "vITh0KVJ10qX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "# this is basically the system prompt\n",
        "alpaca_prompt = \"\"\"Below is instructions in Nepali Language that describes a task, paired with an input that provides further context. Write a response in Nepali language that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions =  examples[\"nepali_instruction\"]\n",
        "    inputs       = examples[\"nepali_input\"]\n",
        "    outputs      = examples[\"nepali_output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('Telugu-LLM-Labs/nepali_alpaca_yahma_cleaned_filtered',split = \"train\")"
      ],
      "metadata": {
        "id": "PXGYy0HWiXIt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e9ebefbe647d4a2d91d1406fd38d3ad5",
            "50823308eb6f4bdd8f6ecda07d79adf7",
            "866c31261d6543228856845ccb161565",
            "026c8ce8547140179f84bd4d3db51c07",
            "376cf1e216644a129ca1b05e6990b868",
            "dc4a7e0c05ba464ba8f6ba2376939b3d",
            "21381cff4dc84fadaec2036ec08bd54c",
            "bd93ea35c9644591b90a1752c53824da",
            "32bc62ce61c5480e9fa7ab0d88429f34",
            "168e34e92f214b77867ea5478a94406b",
            "7618e5bb2ed14dd2b7ac3e7eb505ba7f",
            "22cabaf5ee7640a7b45cbdf1d551f646",
            "756a0dde74f84bd9bca8b207bafea9f9",
            "13313d9f7ed446d181c099bf6cb6807e",
            "770dfafac189446eb5c014702e8e0f23",
            "6e6a974f6db145edb5fd99f3f9bb96f8",
            "ac6d930af7bd4f7aaa518b38bd8080e2",
            "0058bb37ccf14e5296233383b2327e6d",
            "620d333d45d94449bd5487ec4933ab23",
            "c24c07845ce641fc81e04973fbda990f",
            "2a821a377508476fb1f71620599bea61",
            "ae0128ab80e5453ea089fe8d58989fd0",
            "3709b6f8c28b420ba6f441e2b974d17a",
            "e04845877fa942e0be58283e0757604e",
            "6425e903449940e48ab7d402bad78588",
            "665d8d0955ca4be6aa29e3335a634b36",
            "da8d2181898f429ba5ac239f971b9a75",
            "8d4240ead5eb4bcc82cbc0c6d41ddb29",
            "d7cb316ef52e49e8a8e2dc4089cf761c",
            "99d42257154e47549ac401c2959b9e95",
            "08e83b3c4d8e43e8b513ee5f2338237b",
            "e9e6f743a0e84b6bb4e86625c2d1e607",
            "8d9bc31bc13f496789a16e7e3846086f"
          ]
        },
        "outputId": "34b06a43-2314-44e6-e9e3-0e6c5cfc5a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/486 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9ebefbe647d4a2d91d1406fd38d3ad5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/45.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22cabaf5ee7640a7b45cbdf1d551f646"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/28910 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3709b6f8c28b420ba6f441e2b974d17a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ds.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "854ac0a20ae542fba5b324f246faf900",
            "e3b3ba1d831e44dbae99ff69e3347ac9",
            "a470c5becd1f47fc97e583a172fb24d1",
            "1bc850a805b84fd39f4fa22aa43fd64c",
            "070b7df70c0944c685c394dda9604a5b",
            "65b51b82f23b490399c3b5c8e725c25c",
            "f8fdd179e7754b86a0921ace2ef33d37",
            "b5351d1204e14d18bc1cf59ddaabcc28",
            "ae8252625e5d4a4fa6744a0a29c5ca9a",
            "49c604cc83d947709a4b5f4b8caa1b5a",
            "c10adaefdec94641acff3e2bdd0c0169"
          ]
        },
        "id": "xF_2eKwBuYg1",
        "outputId": "9d3ada4b-590f-415f-cc3f-976f93dff473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/28910 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "854ac0a20ae542fba5b324f246faf900"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbP4Rj9VrGR8",
        "outputId": "dd2af34e-9d9f-4dbb-8fa3-0146c7ba44cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Describe the structure of an atom.',\n",
              " 'input': '',\n",
              " 'output': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\",\n",
              " 'nepali_instruction': '‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§',\n",
              " 'nepali_input': '',\n",
              " 'nepali_output': '‡§™‡§∞‡§Æ‡§æ‡§£‡•Å ‡§∏‡§¨‡•à ‡§™‡§¶‡§æ‡§∞‡•ç‡§•‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ñ‡§£‡•ç‡§° ‡§π‡•ã ‡§∞ ‡§Ø‡•ã ‡§§‡•Ä‡§® ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞‡§ï‡§æ ‡§ï‡§£‡§π‡§∞‡•Ç ‡§Æ‡§ø‡§≤‡•á‡§∞ ‡§¨‡§®‡•á‡§ï‡•ã ‡§π‡•Å‡§®‡•ç‡§õ: ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§®, ‡§®‡•ç‡§Ø‡•Ç‡§ü‡•ç‡§∞‡•ã‡§® ‡§∞ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡•§ ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ‡§≤‡§æ‡§à ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§ï‡•ã ‡§¨‡§æ‡§¶‡§≤‡§≤‡•á ‡§ò‡•á‡§∞‡§ø‡§è‡§ï‡•ã ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§Æ‡§æ ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§ï‡•ã ‡§∞‡•Ç‡§™‡§Æ‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡§ø‡§®‡•ç‡§õ‡•§\\n\\n‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§® ‡§∞ ‡§®‡•ç‡§Ø‡•Å‡§ü‡•ç‡§∞‡•ã‡§®‡§Æ‡§ø‡§≤‡•á‡§∞ ‡§¨‡§®‡•á‡§ï‡•ã ‡§π‡•Å‡§®‡•ç‡§õ ‡•§ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§® ‡§π‡§∞‡•Ç ‡§ß‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§Ü‡§µ‡•á‡§∂‡§ø‡§§ ‡§ï‡§£‡§π‡§∞‡•Ç ‡§π‡•Å‡§®‡•ç ‡§∞ ‡§®‡•ç‡§Ø‡•Å‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç ‡§§‡§ü‡§∏‡•ç‡§• ‡§ï‡§£‡§π‡§∞‡•Ç ‡§π‡•Å‡§®‡•ç ‡§ú‡§∏‡§ï‡•ã ‡§ï‡•Å‡§®‡•à ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§π‡•Å‡§Å‡§¶‡•à‡§®‡•§ ‡§Ø‡•Ä ‡§¶‡•Å‡§µ‡•à ‡§ï‡§£‡§π‡§∞‡•Ç ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§Æ‡§æ ‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§õ‡§®‡•ç, ‡§ú‡•Å‡§® ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§Æ‡§æ ‡§õ ‡§∞ ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ ‡§¶‡•ç‡§∞‡§µ‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§∏‡§Æ‡§æ‡§µ‡•á‡§∂ ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§\\n\\n‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§ï‡•ã ‡§µ‡§∞‡§ø‡§™‡§∞‡§ø ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§¨‡§æ‡§¶‡§≤ ‡§õ‡•§ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç ‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞‡•Ç‡§™‡§Æ‡§æ ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§ó‡§∞‡§ø‡§è‡§ï‡§æ ‡§ï‡§£‡§π‡§∞‡•Ç ‡§π‡•Å‡§®‡•ç ‡§ú‡•Å‡§® ‡§®‡•ç‡§Ø‡•Ç‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§ï‡•ã ‡§µ‡§∞‡§ø‡§™‡§∞‡§ø ‡§∏‡•ç‡§•‡§ø‡§∞ ‡§ó‡§§‡§ø‡§Æ‡§æ ‡§π‡•Å‡§®‡•ç‡§õ‡§®‡•ç‡•§ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§® ‡§ï‡•ç‡§≤‡§æ‡§â‡§°‡§≤‡§æ‡§à ‡§ó‡•ã‡§≤‡•á ‡§µ‡§æ ‡§ë‡§∞‡•ç‡§¨‡§ø‡§ü‡§≤‡•ç‡§∏‡§Æ‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ø‡§§ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ, ‡§∞ ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∂‡•á‡§≤‡§≤‡•á ‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡§Æ‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç ‡§∏‡§Æ‡§æ‡§§‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•§ ‡§∏‡§¨‡•à‡§≠‡§®‡•ç‡§¶‡§æ ‡§¨‡§æ‡§π‡§ø‡§∞‡•Ä ‡§ñ‡•ã‡§≤‡§Æ‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ, ‡§ú‡§∏‡§≤‡§æ‡§à ‡§≠‡•ç‡§Ø‡§æ‡§≤‡•á‡§®‡•ç‡§∏ ‡§∂‡•á‡§≤ ‡§≠‡§®‡§ø‡§®‡•ç‡§õ, ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§∞‡§æ‡§∏‡§æ‡§Ø‡§®‡§ø‡§ï ‡§ó‡•Å‡§£‡§π‡§∞‡•Ç ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§£ ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§ \\n\\n‡§è‡§ï ‡§§‡§ü‡§∏‡•ç‡§• ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§Æ‡§æ, ‡§®‡•ç‡§Ø‡•Ç‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§Æ‡§æ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§®‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§® ‡§ï‡•ç‡§≤‡§æ‡§â‡§°‡§Æ‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§¨‡§∞‡§æ‡§¨‡§∞ ‡§π‡•Å‡§®‡•ç‡§õ, ‡§§‡•ç‡§Ø‡§∏‡•à‡§≤‡•á ‡§ß‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞ ‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ö‡§æ‡§∞‡•ç‡§ú‡§π‡§∞‡•Ç ‡§∏‡§®‡•ç‡§§‡•Å‡§≤‡§®‡§Æ‡§æ ‡§π‡•Å‡§®‡•ç‡§õ‡§®‡•ç ‡§∞ ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§Æ‡§æ ‡§ï‡•Å‡§®‡•à ‡§∏‡§Æ‡§ó‡•ç‡§∞ ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§π‡•Å‡§Å‡§¶‡•à‡§®‡•§ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§®‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ, ‡§ú‡§∏‡§≤‡§æ‡§à ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§™‡§®‡§ø ‡§≠‡§®‡§ø‡§®‡•ç‡§õ, ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å ‡§ï‡•Å‡§® ‡§§‡§§‡•ç‡§µ ‡§π‡•ã ‡§≠‡§®‡•á‡§∞ ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§£ ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§',\n",
              " 'text': 'Below is instructions in Nepali Language that describes a task, paired with an input that provides further context. Write a response in Nepali language that appropriately completes the request.\\n\\n### Instruction:\\n‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§\\n\\n### Input:\\n\\n\\n### Response:\\n‡§™‡§∞‡§Æ‡§æ‡§£‡•Å ‡§∏‡§¨‡•à ‡§™‡§¶‡§æ‡§∞‡•ç‡§•‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§≠‡•Ç‡§§ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ñ‡§£‡•ç‡§° ‡§π‡•ã ‡§∞ ‡§Ø‡•ã ‡§§‡•Ä‡§® ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞‡§ï‡§æ ‡§ï‡§£‡§π‡§∞‡•Ç ‡§Æ‡§ø‡§≤‡•á‡§∞ ‡§¨‡§®‡•á‡§ï‡•ã ‡§π‡•Å‡§®‡•ç‡§õ: ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§®, ‡§®‡•ç‡§Ø‡•Ç‡§ü‡•ç‡§∞‡•ã‡§® ‡§∞ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡•§ ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ‡§≤‡§æ‡§à ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§ï‡•ã ‡§¨‡§æ‡§¶‡§≤‡§≤‡•á ‡§ò‡•á‡§∞‡§ø‡§è‡§ï‡•ã ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§Æ‡§æ ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§ï‡•ã ‡§∞‡•Ç‡§™‡§Æ‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡§ø‡§®‡•ç‡§õ‡•§\\n\\n‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§® ‡§∞ ‡§®‡•ç‡§Ø‡•Å‡§ü‡•ç‡§∞‡•ã‡§®‡§Æ‡§ø‡§≤‡•á‡§∞ ‡§¨‡§®‡•á‡§ï‡•ã ‡§π‡•Å‡§®‡•ç‡§õ ‡•§ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§® ‡§π‡§∞‡•Ç ‡§ß‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§Ü‡§µ‡•á‡§∂‡§ø‡§§ ‡§ï‡§£‡§π‡§∞‡•Ç ‡§π‡•Å‡§®‡•ç ‡§∞ ‡§®‡•ç‡§Ø‡•Å‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç ‡§§‡§ü‡§∏‡•ç‡§• ‡§ï‡§£‡§π‡§∞‡•Ç ‡§π‡•Å‡§®‡•ç ‡§ú‡§∏‡§ï‡•ã ‡§ï‡•Å‡§®‡•à ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§π‡•Å‡§Å‡§¶‡•à‡§®‡•§ ‡§Ø‡•Ä ‡§¶‡•Å‡§µ‡•à ‡§ï‡§£‡§π‡§∞‡•Ç ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§Æ‡§æ ‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§õ‡§®‡•ç, ‡§ú‡•Å‡§® ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§Æ‡§æ ‡§õ ‡§∞ ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ ‡§¶‡•ç‡§∞‡§µ‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§∏‡§Æ‡§æ‡§µ‡•á‡§∂ ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§\\n\\n‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§®‡•ç‡§Ø‡•Å‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§ï‡•ã ‡§µ‡§∞‡§ø‡§™‡§∞‡§ø ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§¨‡§æ‡§¶‡§≤ ‡§õ‡•§ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç ‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞‡•Ç‡§™‡§Æ‡§æ ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§ó‡§∞‡§ø‡§è‡§ï‡§æ ‡§ï‡§£‡§π‡§∞‡•Ç ‡§π‡•Å‡§®‡•ç ‡§ú‡•Å‡§® ‡§®‡•ç‡§Ø‡•Ç‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§ï‡•ã ‡§µ‡§∞‡§ø‡§™‡§∞‡§ø ‡§∏‡•ç‡§•‡§ø‡§∞ ‡§ó‡§§‡§ø‡§Æ‡§æ ‡§π‡•Å‡§®‡•ç‡§õ‡§®‡•ç‡•§ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§® ‡§ï‡•ç‡§≤‡§æ‡§â‡§°‡§≤‡§æ‡§à ‡§ó‡•ã‡§≤‡•á ‡§µ‡§æ ‡§ë‡§∞‡•ç‡§¨‡§ø‡§ü‡§≤‡•ç‡§∏‡§Æ‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ø‡§§ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ, ‡§∞ ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∂‡•á‡§≤‡§≤‡•á ‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡§Æ‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç ‡§∏‡§Æ‡§æ‡§§‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•§ ‡§∏‡§¨‡•à‡§≠‡§®‡•ç‡§¶‡§æ ‡§¨‡§æ‡§π‡§ø‡§∞‡•Ä ‡§ñ‡•ã‡§≤‡§Æ‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ, ‡§ú‡§∏‡§≤‡§æ‡§à ‡§≠‡•ç‡§Ø‡§æ‡§≤‡•á‡§®‡•ç‡§∏ ‡§∂‡•á‡§≤ ‡§≠‡§®‡§ø‡§®‡•ç‡§õ, ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§ï‡•ã ‡§∞‡§æ‡§∏‡§æ‡§Ø‡§®‡§ø‡§ï ‡§ó‡•Å‡§£‡§π‡§∞‡•Ç ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§£ ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§ \\n\\n‡§è‡§ï ‡§§‡§ü‡§∏‡•ç‡§• ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§Æ‡§æ, ‡§®‡•ç‡§Ø‡•Ç‡§ï‡•ç‡§≤‡§ø‡§Ø‡§∏‡§Æ‡§æ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§®‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§® ‡§ï‡•ç‡§≤‡§æ‡§â‡§°‡§Æ‡§æ ‡§á‡§≤‡•á‡§ï‡•ç‡§ü‡•ç‡§∞‡•ã‡§®‡§π‡§∞‡•Ç‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§¨‡§∞‡§æ‡§¨‡§∞ ‡§π‡•Å‡§®‡•ç‡§õ, ‡§§‡•ç‡§Ø‡§∏‡•à‡§≤‡•á ‡§ß‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞ ‡§®‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ö‡§æ‡§∞‡•ç‡§ú‡§π‡§∞‡•Ç ‡§∏‡§®‡•ç‡§§‡•Å‡§≤‡§®‡§Æ‡§æ ‡§π‡•Å‡§®‡•ç‡§õ‡§®‡•ç ‡§∞ ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å‡§Æ‡§æ ‡§ï‡•Å‡§®‡•à ‡§∏‡§Æ‡§ó‡•ç‡§∞ ‡§ö‡§æ‡§∞‡•ç‡§ú ‡§π‡•Å‡§Å‡§¶‡•à‡§®‡•§ ‡§™‡•ç‡§∞‡•ã‡§ü‡•ã‡§®‡§ï‡•ã ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ, ‡§ú‡§∏‡§≤‡§æ‡§à ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§™‡§®‡§ø ‡§≠‡§®‡§ø‡§®‡•ç‡§õ, ‡§™‡§∞‡§Æ‡§æ‡§£‡•Å ‡§ï‡•Å‡§® ‡§§‡§§‡•ç‡§µ ‡§π‡•ã ‡§≠‡§®‡•á‡§∞ ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§£ ‡§ó‡§∞‡•ç‡§¶‡§õ‡•§<|end_of_text|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "453puIIdsXb0",
        "outputId": "38e6832f-220e-41fb-9c1a-1b637f6110c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "    num_rows: 28910\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsos47V-tNeJ",
        "outputId": "e37ffeaf-aeb3-4d17-ff9d-e3b0ef04f2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "        num_rows: 23128\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "        num_rows: 5782\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBC4P6nE1mdU",
        "outputId": "55c75923-d385-491d-8cb0-36ca683fc7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "        num_rows: 23128\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "        num_rows: 5782\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset[\"train\"] # Access the training split using the key 'train'\n",
        "eval_dataset = dataset[\"test\"]   # Access the testing split using the key 'test'\n",
        "print(train_dataset)             # Print the training dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHCAhyCc1A5v",
        "outputId": "ace97444-03bb-4e26-9a28-f72428edf445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
            "    num_rows: 23128\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "1Kw9I15UGGUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On orginal tokenizer"
      ],
      "metadata": {
        "id": "OHHljMxJob2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "- We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
        "- At this stage, we're configuring our model's training setup, where we define things like batch size and learning rate, to teach our model effectively with the data we have prepared."
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "89c9c546f4dc41ce8129d78f05eb0736",
            "1ae3981d20ca4d8e933e0c4597c2a13c",
            "00912c7cbd304ef7a42ed5139c181deb",
            "c99ca5112cc24a35bbc2d37642fa91cf",
            "8a8c01169c554127b359373f1dd830af",
            "31f98db16ad14442a47dce7fbf00ea6f",
            "f3855c1be9b141b88020d4a21b6a9fd0",
            "17c1f1f7f0a0457f89ba80273b231bb0",
            "7a7f445c673f42eca486a2954d5c9019",
            "cfed8a66501546a2a08a5d05cb3a6e30",
            "3a41b68a2fa740d4847582ece34087e4"
          ]
        },
        "outputId": "faef7e62-934e-4401-99d2-f5705765a239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89c9c546f4dc41ce8129d78f05eb0736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    # eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True, # Moved packing argument here\n",
        "    args = TrainingArguments(\n",
        "      per_device_train_batch_size = 2,\n",
        "      gradient_accumulation_steps = 4,\n",
        "      max_steps = 5,\n",
        "      # eval_steps = 10,\n",
        "      # save_strategy = \"steps\",\n",
        "      # save_steps = 1,\n",
        "      # save_total_limit = 3,\n",
        "      # logging_strategy = \"steps\",\n",
        "      warmup_steps = 5,\n",
        "      # eval_strategy = \"steps\",\n",
        "      # do_eval= True,\n",
        "      # label_names = [\"labels\"],\n",
        "      learning_rate = 2e-4,\n",
        "      fp16 = not is_bfloat16_supported(),\n",
        "      bf16 = is_bfloat16_supported(),\n",
        "      logging_steps = 1,\n",
        "      optim = \"adamw_8bit\",\n",
        "      weight_decay = 0.01,\n",
        "      lr_scheduler_type = \"cosine\",\n",
        "      seed = 3407,\n",
        "      output_dir = \"outputs\",\n",
        "      # load_best_at_end = True,\n",
        "      # metric_for_best_model = \"eval_loss\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add padding and truncation to the tokenizer\n",
        " # Set padding token if not already set\n",
        "trainer.args.padding = True  # Enable padding\n",
        "trainer.args.truncation = True # Enable truncation"
      ],
      "metadata": {
        "id": "BOpmS040_nXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f87b79d-25cf-4015-f0d2-0074e5aee51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "5.605 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY-Ex3H3uDP1",
        "outputId": "7e02ec8b-bb70-424a-b9d8-0d04e4ce60b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "8c4206c1-5aa3-49d6-af5c-63bfcc3f57cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 6,572 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 5\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 03:05, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.758600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.695600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.631900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.517100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# We're now kicking off the actual training of our model, which will spit out some statistics showing us how well it learns\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eefc4b-cb60-4b7f-8ccd-76f380449fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "233.6979 seconds used for training.\n",
            "3.89 minutes used for training.\n",
            "Peak reserved memory = 10.436 GB.\n",
            "Peak reserved memory for training = 4.831 GB.\n",
            "Peak reserved memory % of max memory = 70.762 %.\n",
            "Peak reserved memory for training % of max memory = 32.757 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ],
      "metadata": {
        "id": "ekOmTR1hSNcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\", # input\n",
        "        \"‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§∞‡§π‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§§‡•Ä‡§® ‡§µ‡§ü‡§æ ‡§ü‡§ø‡§™‡•ç‡§∏ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡•§\",\n",
        "        \"\",\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "kR3gIAX-SM2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fb3343-fd53-4ae3-e797-33c556c08d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is instructions in Nepali Language that describes a task, paired with an input that provides further context. Write a response in Nepali language that appropriately completes the request.\\n\\n### Instruction:\\n\\n\\n### Input:\\n‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§∞‡§π‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§§‡•Ä‡§® ‡§µ‡§ü‡§æ ‡§ü‡§ø‡§™‡•ç‡§∏ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡•§\\n\\n### Response:\\n1. ‡§∞‡§æ‡§§‡§Æ‡§æ ‡§™‡§æ‡§Å‡§ö ‡§ò‡§£‡•ç‡§ü‡§æ ‡§®‡§ø‡§¶‡•ç‡§∞‡§æ ‡§≤‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡•§\\n2. ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® 30 ‡§Æ‡§ø‡§®‡§ø‡§ü‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§ï‡§∏‡§∞‡•Ä ‡§ö‡§≤‡•ç‡§®‡•á ‡§π‡•ã?\\n3. ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® 30 ‡§Æ‡§ø‡§®‡§ø‡§ü‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§ï‡§∏‡§∞‡•Ä ‡§ö‡§≤‡•ç‡§®‡•á ‡§π‡•ã?<|end_of_text|>']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ],
      "metadata": {
        "id": "CrSvZObor0lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize list to hold training and evaluation losses and steps\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "train_step = []\n",
        "eval_step = []\n",
        "\n",
        "#Populate the list from log history\n",
        "#import pandas as pd\n",
        "# pd.DataFrame(trainer.state.log_history)\n",
        "for entry in trainer.state.log_history:\n",
        "  if 'loss' in entry:\n",
        "    train_losses.append(entry['loss'])\n",
        "    train_step.append(entry['step'])\n",
        "  if 'eval_loss' in entry:\n",
        "    eval_losses.append(entry['eval_loss'])\n",
        "    eval_step.append(entry['step'])\n",
        "\n",
        "# plot the losses\n",
        "plt.plot(train_step, train_losses, label = 'Train Loss')\n",
        "plt.plot(eval_step, eval_losses, label = 'eval Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gBxsBdA9kUGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "7b574cfc-d286-4bad-8e42-b3ad3beebe8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ],
      "metadata": {
        "id": "uMuVrWbjAzhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ],
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ],
      "metadata": {
        "id": "bDp0zNpwe6U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On Custom tokenizer\n",
        "\n",
        "- I train the tokenizer on Nepali corpus and increase the vocab length from 128k to 140k"
      ],
      "metadata": {
        "id": "MuPE-lXooyvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl huggingface_hub"
      ],
      "metadata": {
        "id": "m01kZdYQpC5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXpH5x8kT3Yo"
      },
      "source": [
        "## Step 1: Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5iWjARxiZ2n"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqudjJzzU_H-"
      },
      "source": [
        "## Step 3: import Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjHLmplUYxhK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer,SFTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUHPCJEjGe8"
      },
      "source": [
        "## Step 4: load everything and start fine-tunning process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Rajeeb321/unsloth-llama-3-8b-bnb-4bit-Nepali-tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242,
          "referenced_widgets": [
            "915d1dc8557448a0956e7598b94d2130",
            "7e121474d20844a8a67a3c67c4db39a3",
            "161a4078b11246a08c27b079c955d671",
            "8ebb8122060045daaf5c0865c7e9bd3a",
            "051bfa0e181c418eab5d209acd234ecc",
            "b608339bde1746f5bb25941cfcf8ef9b",
            "ec986c7d7ebb4ae6ba401012e42bb14f",
            "6d28112ecf3f41a89cafdf2c2807f61d",
            "0c33fb96f961453aa60cb482f8496ef4",
            "9384669e63b94f97a68d89d5050f38e6",
            "9fa0ee8bd344445f92fed3df18f2e7db",
            "2880104df2a1426da6e7dcdf4d6b0efe",
            "1b6fa89130234c8bbb193fb5c3240d2d",
            "342a74a767264262ba934373284679d5",
            "de408184771943328b200ed0d66a2ea9",
            "2de4b0c88315477793885ef9d5c0b914",
            "ff21a6556e25430eafcdd7b80aff5cc2",
            "700091fb00014059bebb40601a0355a5",
            "4fe17daf870541fca9001905b7fe51d0",
            "3d1aa21d35364dc1b239879ef82e890a",
            "f2cdcdc2d7764f7497d86f9b15d8b158",
            "0dac40c275a240ce8f531cac5eacfe61",
            "89edf691e8ac4a7ea4a1af2daa3a4839",
            "19eb56f42a9b42309589629063eb8b01",
            "c063250f49264eaea18c0c2a67d01c81",
            "b5439010232e4f3dab85b9a9bd641aea",
            "3981c4675d2e480b918db888cf0d3c4b",
            "ee28e2dfd7e34f18aa8b01d6178d2ee5",
            "8a3bce77c5374a109a39761f52ba1067",
            "6e137065c3fc4f35b54f92c09adc6a8a",
            "ed2d19e3e2544c9c9751f4a4eb4ca8c5",
            "3e44a0a19b9e4c409d02941b3cfaae65",
            "e06fc8efb25d46ecb2cd311c5bf671d0"
          ]
        },
        "id": "V3RJPGUXgRt9",
        "outputId": "fa821f51-4a32-485e-f817-de2f71d8c4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "915d1dc8557448a0956e7598b94d2130"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2880104df2a1426da6e7dcdf4d6b0efe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89edf691e8ac4a7ea4a1af2daa3a4839"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTRldGGggZd",
        "outputId": "e64dc776-45de-42c2-f4e8-4f1a2f0c1f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8POXdxWZVYyA"
      },
      "outputs": [],
      "source": [
        "# this is basically the system prompt\n",
        "alpaca_prompt = \"\"\"Below is instructions in Nepali Language that describes a task, paired with an input that provides further context. Write a response in Nepali language that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions =  examples[\"nepali_instruction\"]\n",
        "    inputs       = examples[\"nepali_input\"]\n",
        "    outputs      = examples[\"nepali_output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "F71hfDufg-mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GlGahe3pnFbT",
        "outputId": "bd126977-7681-49f1-d332-18362370c9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|end_of_text|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e3251f7669e64ff6ae5c25963a6402d0",
            "b115cb3903264eecb46816f94b7e8f58",
            "8168a3746db3450f81f30b0a738ad3e9",
            "6504c9404e654d1cab1e29509eb59886",
            "5f726c59a5b84c088b17204bc4e5c991",
            "8cd00975fc214b43926f83b28ff03815",
            "952e2847f5c242f88bdc04b74324dc43",
            "5f2da69694a54b6b85e841b249d4ba0f",
            "96f241ea15474008971d85d49d01f1aa",
            "342790aaad2544369e51b362ada83ca5",
            "a5fb85d5c6754993a298d072de93b042",
            "ee05348b2c6a440a9a2d30ed5644d1de",
            "941f50fa79604306947d8532d6b22392",
            "502e68b0cdb744408b8395801df2e8b6",
            "d6719066e0944382a5b0e8b6622d9def",
            "daf14d543a5d413682d1d1b66612b5df",
            "89e2916c9a5e45989d1ed9cce1afae3e",
            "25341e35aa914cef90f89f2d359f86f7",
            "2e760bd6eec14fa5ae5e007f5b7d9d9c",
            "fce970d9282b41ea8dd9a06b607ffcf2",
            "661255b2400f458f9cd9092720f7a87f",
            "e995fb1237e74194b56bcbd0f11256f8",
            "2aff9fccaca64c0c8411f9047f24bf02",
            "3531305876004d05ba23cd484b70e735",
            "ab343d8422f84eb89f8573701dd6a7d9",
            "4da4ffdb5602474e9dd76cfdbfe81919",
            "b35930f1c39643c4a5a290970be272c4",
            "705015df3d9e4b79a199a882aedd4372",
            "734ab616db4942f2a7ff8ac2475456d1",
            "4f1aeae1e2dd403eb3c94157fa7bd6a4",
            "4fad93e644794a77996870291cfc2671",
            "37f6e5dc61a44531a1de123c929c7c8a",
            "de9b78fedc4e4558b06e7ff10e40f194"
          ]
        },
        "id": "FHPV-KI1jPfs",
        "outputId": "d33d6e27-43fc-4d7a-884b-20514227103a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/486 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3251f7669e64ff6ae5c25963a6402d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/45.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee05348b2c6a440a9a2d30ed5644d1de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/28910 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2aff9fccaca64c0c8411f9047f24bf02"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('Telugu-LLM-Labs/nepali_alpaca_yahma_cleaned_filtered',split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fc4d576a55174ca994e84cfeb245c8a2",
            "2d989af475f84637ae7f54f30d386261",
            "c45628ace8c4454aa73511cd003b37df",
            "a5fc5e4aff34482d8ff100ac9dd340e2",
            "e0cb7dbd277c4a4881334323c0e79478",
            "3881d52d06104ae2902348ab95e08b77",
            "4f0d0933ae5343dea22580d0427bd661",
            "d0a4c46752df4b00b830eae1f3da2cfc",
            "326d2a99956f4b459c6201d2f1324bc6",
            "56f85e014069479abb05a94300404f79",
            "b2d7e5e9a32548d485b75c48374359b8"
          ]
        },
        "id": "WpqfTgWU3nV_",
        "outputId": "13c76486-3f0d-46c0-bffc-61f8011937d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/28910 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4d576a55174ca994e84cfeb245c8a2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = ds.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Nnstr2g1sc",
        "outputId": "633b0273-130c-4a08-8aaf-5ad3c57fc4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "        num_rows: 23128\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
              "        num_rows: 5782\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset[\"train\"] # Access the training split using the key 'train'\n",
        "eval_dataset = dataset[\"test\"]   # Access the testing split using the key 'test'\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96gbmQYg6XA",
        "outputId": "8cdfdfc9-4a55-438d-93ad-05e7860bdb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'input', 'output', 'nepali_instruction', 'nepali_input', 'nepali_output', 'text'],\n",
            "    num_rows: 23128\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxKpcyMB3i6U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187,
          "referenced_widgets": [
            "fca9c3f0297f4cfe998087b3db768dde",
            "74ee359d63ed4237ba25e838e0b37fb7",
            "2e58523c07094f6fb1c5fa3fd176a05f",
            "42b2d4978c264062987e36c19e19a4b0",
            "0ede6b938aa94348b1e53303f8d0ec6e",
            "b596b7bdcb4f46949d059c480e044167",
            "7b603f1ea9354da3bd0590d1629ccb2a",
            "d37299b26e464b33a9206fe612a7ce1a",
            "019aeaf16a2e4e3c9db91176758f5de0",
            "742b99284abd48e99675d745f991e2b3",
            "7a3b6d900c5e4b6c9cd88730b72e8db5",
            "5d3e8ccaca6443658b164ac4ee98b715",
            "41ec3ce212064b4f8b9c23e8b56c9e1d",
            "bc7647a54995437da7b61b28b30df843",
            "e80ec493970a423680286b5745fee506",
            "4176ac481c3840cd91c19f7f4f32429b",
            "c9b462620cb44d89b0e5447465bdfc4e",
            "f9d04894c6e141829f634cacb93580bf",
            "cbd4d748739341e0bd6759301d46b17e",
            "8b34cd86289a4f54993b1b17c141815c",
            "19d66e6681c64a649120434ffcf0a633",
            "669e46af684a4de08e91257640567139",
            "f05ba7e9a2374c3fa290da8baed4cadf",
            "9b10fa05702d4a56bdabd9ba9e56fdf4",
            "11f699ffab2f42369d688e3a481a0958",
            "76d9d8e085434dbfb84e7193f75bb725",
            "1337687c596040bebe6e56d3c17b152f",
            "86d19d577b1744b3b82d7a1ad4e11f13",
            "7440ac79b1694502aa43e3649a47ee74",
            "cdf6fcb0449940d9b7b6b36c091f5628",
            "7ea0af90d3304b9ea8b7c22e4013a0a9",
            "ad3624bd5d5e4ff2b638fb7efd15ee6d",
            "757f0b39e3b9430190f13736863d5b08"
          ]
        },
        "outputId": "590148cc-c89f-41b5-cd7c-8892a5029d26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca9c3f0297f4cfe998087b3db768dde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d3e8ccaca6443658b164ac4ee98b715"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f05ba7e9a2374c3fa290da8baed4cadf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6ziWNR-h9mK",
        "outputId": "3f2f736b-8e93-461e-8cc3-59c5eb0ec120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(140000, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    # eval_dataset = eval_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True, # Moved packing argument here\n",
        "    args = TrainingArguments(\n",
        "      per_device_train_batch_size = 2,\n",
        "      gradient_accumulation_steps = 4,\n",
        "      max_steps = 5,\n",
        "      # eval_steps = 10,\n",
        "      # save_strategy = \"steps\",\n",
        "      # save_steps = 1,\n",
        "      # save_total_limit = 3,\n",
        "      # logging_strategy = \"steps\",\n",
        "      warmup_steps = 5,\n",
        "      # eval_strategy = \"steps\",\n",
        "      # do_eval= True,\n",
        "      # label_names = [\"labels\"],\n",
        "      learning_rate = 2e-4,\n",
        "      fp16 = True,\n",
        "      bf16 = False,\n",
        "      logging_steps = 1,\n",
        "      optim = \"adamw_8bit\",\n",
        "      weight_decay = 0.01,\n",
        "      lr_scheduler_type = \"cosine\",\n",
        "      seed = 3407,\n",
        "      output_dir = \"outputs\",\n",
        "      # load_best_at_end = True,\n",
        "      # metric_for_best_model = \"eval_loss\",\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "1576fb78e5984cf5a4102e0aad9649c5",
            "0ff4b442935d436c92501f56a64c70ba",
            "bee9c3762b5845eca7cda7ddcc6f2384",
            "0764fc919e1b403182983c15b0e3c66c",
            "6ad72c294caa49c0845a9b6de37c6f52",
            "a8f6d93da8ad4295bad2478d9365a3fe",
            "60be29eb9f25444e8afe235e6813ccc9",
            "7fe132bc37404adbac68d10de7fefd94",
            "3948ccf67d2d4688a4e20119212a4916",
            "6484902247da4455a309d80e1986c363",
            "a303e534eb5c4807a39a17259c7dcfc2"
          ]
        },
        "id": "I8HQBxvzgId3",
        "outputId": "074e065a-aa7d-4d67-e61e-f5723223ba5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_num_proc, packing. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1576fb78e5984cf5a4102e0aad9649c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nLKFWsZkMHQ",
        "outputId": "cb032347-b2b7-467d-d082-812ea946c418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "9.252 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHb0j0MMlSGY",
        "outputId": "342edc91-370a-48b5-b9a4-0d0e3bf9acf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17opDevZS79L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "41a0627a-9c10-48d5-dda2-159a2caac004"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3d62c575fcfd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3264\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3265\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1431\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1165\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    966\u001b[0m                 )\n\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    969\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\", # input\n",
        "        \"‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§∞‡§π‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø ‡§§‡•Ä‡§® ‡§µ‡§ü‡§æ ‡§ü‡§ø‡§™‡•ç‡§∏ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç ‡•§\",\n",
        "        \"\",\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "ox2wo9usp_-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize list to hold training and evaluation losses and steps\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "train_step = []\n",
        "eval_step = []\n",
        "\n",
        "#Populate the list from log history\n",
        "#import pandas as pd\n",
        "# pd.DataFrame(trainer.state.log_history)\n",
        "for entry in trainer.state.log_history:\n",
        "  if 'loss' in entry:\n",
        "    train_losses.append(entry['loss'])\n",
        "    train_step.append(entry['step'])\n",
        "  if 'eval_loss' in entry:\n",
        "    eval_losses.append(entry['eval_loss'])\n",
        "    eval_step.append(entry['step'])\n",
        "\n",
        "# plot the losses\n",
        "plt.plot(train_step, train_losses, label = 'Train Loss')\n",
        "plt.plot(eval_step, eval_losses, label = 'eval Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pqSzr5djqP3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ],
      "metadata": {
        "id": "uDogbvSDqgWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "hSX6zqx9qgWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ],
      "metadata": {
        "id": "UnCT_O8LqgWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}